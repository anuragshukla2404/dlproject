{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a href=\"https://cocl.us/DL0320EN_TOP_IMAGE\">\n","    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Assets/Images/Top.png\" width=\"750\" alt=\"IBM 10TB Storage\">\n","</a>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h1>Classifying European Money Denominations: Training a Pre-trained model  </h1>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h2>Table of Contents</h2>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p>In this lab, you will train the pre-trained model to classify the European currency. You will use the dataset object you created in the previous lab.</p>\n","<ul>\n","    <li><a href=\"#gen\">Create Image Dataset Generator</a></li>\n","    <li><a href=\"#ques\">Questions</a>\n","        <ol>\n","            <li><a href=\"q31\">Question 3.1: Preparation</a></li>\n","            <li><a href=\"q32\">Question 3.2: Train the model</a></li>\n","            <li><a href=\"q33\">Question 3.3: Plot 5 Random Images with their predictions</a></li>\n","            <li><a href=\"q34\">Question 3.4: Use the second model <code>Densenet121</code> to do the prediction</a></li>\n","        </ol>\n","    </li>\n","    <li><a href=\"#save\">Save the trained model</a></li>\n","</ul>\n","\n","<p>Estimated Time Needed: <b>60 mins</b></p>\n","<hr>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h2>Preparation</h2>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a href=\"https://cocl.us/DL0320EN_storage\">\n","    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Assets/Images/ObjectStorage.png\" width=\"750\" alt=\"cognitive class\">\n","</a>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Download the datasets you needed for this lab.\n"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["'wget' is not recognized as an internal or external command,\n","operable program or batch file.\n","tar: Error opening archive: Failed to open '/resources/data/training_data_pytorch.tar.gz'\n","'wget' is not recognized as an internal or external command,\n","operable program or batch file.\n","tar: Error opening archive: Failed to open '/resources/data/validation_data_pytorch.tar.gz'\n"]}],"source":["# You can comment out this box when you already have the dataset\n","# Step 1: Ctrl + A : Select all\n","# Step 2: Ctrl + / : Comment out all; if everything selected has been comment out alreaday, then uncomment all\n","\n","# Download Training Dataset\n","!wget --quiet -O /resources/data/training_data_pytorch.tar.gz https://cocl.us/DL0320EN_TRAIN_TAR_PYTORCH\n","!tar -xzf  /resources/data/training_data_pytorch.tar.gz -C /resources/data --exclude '.*'\n","\n","# Download Validation Dataset\n","!wget --quiet -O /resources/data/validation_data_pytorch.tar.gz https://cocl.us/DL0320EN_VALID_TAR_PYTORCH\n","!tar -xzf  /resources/data/validation_data_pytorch.tar.gz -C /resources/data --exclude '.*'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Import the PyTorch Modules needed in the lab.\n"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'tqdm'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[1;32mIn[81], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnn\u001b[39;00m\n\u001b[0;32m      9\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtqdm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnotebook\u001b[39;00m \u001b[39mimport\u001b[39;00m tqdm\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"]}],"source":["# Import PyTorch Modules will be used in the lab\n","\n","import torch \n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","import pandas\n","from torchvision import transforms\n","import torch.nn as nn\n","torch.manual_seed(0)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Import Non-PyTorch Modules \n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Import Non-PyTorch Modules will be used in the lab\n","\n","import time\n","from imageio import imread\n","from matplotlib.pyplot import imshow\n","import matplotlib.pylab as plt\n","import pandas as pd\n","from PIL import Image, ImageDraw, ImageFont\n","import random\n","import numpy as np"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<hr>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h2 id=\"gen\">Create Dataset Class and Object</h2>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this section, you use the dataset class from the last section.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The denomination, file name and the class variable for the training and validation data are stored in the following csv file.\n"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["# Url that contains CSV files\n","\n","train_csv_file = 'https://cocl.us/DL0320EN_TRAIN_CSV'\n","validation_csv_file = 'https://cocl.us/DL0320EN_VALID_CSV'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The training images and validation images  are stored in the following directories.\n"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Absolute path for finding the directory contains image datasets\n","\n","train_data_dir = 'data/training_data_pytorch/'\n","validation_data_dir = 'data/validation_data_pytorch/'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Use  the dataset class you created in the last lab. You can cut and paste it to here:\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["# Create Dateaset Class\n","\n","class Dataset(Dataset):\n","    \n","    # Constructor\n","    def __init__(self, csv_file, data_dir, transform=None):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.data_name = pd.read_csv(csv_file)\n","        self.len = self.data_name.shape[0] \n","    \n","    # Get Length\n","    def __len__(self):\n","        return self.len\n","    \n","    # Getter\n","    def __getitem__(self, idx):\n","        img_name = self.data_dir + self.data_name.iloc[idx, 2]\n","        image = Image.open(img_name)\n","        y = self.data_name.iloc[idx, 3]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, y"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Use the constructor <code>compose</code> to perform the following sequence of transformations in the order they are given, call the object <code>composed</code>\n"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["# Construct the composed object for transforming the image\n","\n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","composed = transforms.Compose([transforms.Resize((224, 224))\n","                               , transforms.ToTensor()\n","                               , transforms.Normalize(mean, std)])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Create a training dataset and validation dataset object using the csv file stored in the variables the <code>train_csv_file</code> and <code>validation_csv_file</code>. The directories are stored in the variable <code>train_data_dir</code> and <code>validation_data_dir</code>. Set the parameter <code>transform</code> to the object <code>composed</code>. \n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# Create the train dataset and validation dataset\n","\n","train_dataset = Dataset(transform=composed\n","                        ,csv_file=train_csv_file\n","                        ,data_dir=train_data_dir)\n","\n","validation_dataset = Dataset(transform=composed\n","                          ,csv_file=validation_csv_file\n","                          ,data_dir=validation_data_dir)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<hr>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h2 id=\"ques\">Questions</h2>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h3 id=\"q31\">Question 3.1: Preparation</h3><b>5 points</b>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Step 1</b>: Load the pre-trained model <code>resnet18</code> Set the parameter <code>pretrained</code> to true.\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\vinod\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","c:\\Users\\vinod\\anaconda3\\envs\\myenv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n"]}],"source":["# Step 1: Load the pre-trained model resnet18\n","\n","# Type your code here\n","model = models.resnet18(pretrained=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Step 2</b>: The following lines of code will set the attribute <code>requires_grad</code> to <code>False</code>. As a result, the parameters will not be affected by training.\n"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# Step 2: Set the parameter cannot be trained for the pre-trained model\n","\n","# Type your code here\n","model.requires_grad_=False"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<code>resnet18</code> is used to classify 1000 different objects; as a result, the last layer has 1000 outputs.  The 512 inputs come from the fact that the previously hidden layer has 512 outputs. \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Step 3</b>: Replace the output layer <code>model.fc</code> of the neural network with a <code>nn.Linear</code> object, to classify 7 different bills. For the parameters <code>in_features </code> remember the last hidden layer has 512 neurons.\n"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["# Step 3: Re-defined the last layer\n","\n","# Type your code here\n","model.fc = nn.Linear(in_features=512,out_features=7)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Print out the model in order to show whether you get the correct answer.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=7, bias=True)\n",")\n"]}],"source":["# Print the model (PLEASE DO NOT MODIFY THIS BOX)\n","\n","print(model)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h3 id=\"q32\">Question 3.2: Train the model</h3><b>5 points</b>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Step 1</b>: Create a cross entropy criterion function \n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# Step 1: Create the loss function\n","\n","# Type your code here\n","loss = nn.CrossEntropyLoss()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Step 2</b>: Create a training loader and validation loader object, the batch size is <i>15</i> and <i>10</i> respectively .\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["# Step 2: Create the data loader\n","\n","# Type your code here\n","train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=15)\n","validation_loader = torch.utils.data.DataLoader(validation_dataset,batch_size=10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Step 3</b>: Use the following optimizer to minimize the loss \n"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["# Step 3: Use the pre-defined optimizer Adam with learning rate 0.003\n","\n","# Type your code here\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.003)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Step 4</b>: Train the model for 20 epochs, save the loss in a list as will as the accuracy on the validation data for every epoch. The entire process may take 6.5 minutes. Print the validation accuracy for each epoch during the epoch loop. Then, plot the training loss for each epoch and validation error for each epoch.\n"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epochs:1 Validation Loss: 0.08571428571428572 Accuracy: (0.0, 8.571428571428571)\n","Epochs:2 Validation Loss: 0.17142857142857143 Accuracy: (0.0, 17.142857142857142)\n","Epochs:3 Validation Loss: 0.2571428571428571 Accuracy: (0.0, 25.714285714285715)\n","Epochs:4 Validation Loss: 0.34285714285714286 Accuracy: (0.0, 34.285714285714285)\n","Epochs:5 Validation Loss: 0.42857142857142855 Accuracy: (0.0, 42.857142857142854)\n","Epochs:6 Validation Loss: 0.5142857142857142 Accuracy: (0.0, 51.42857142857143)\n","Epochs:7 Validation Loss: 0.6 Accuracy: (0.0, 60.0)\n","Epochs:8 Validation Loss: 0.6857142857142857 Accuracy: (0.0, 68.57142857142857)\n","Epochs:9 Validation Loss: 0.7714285714285715 Accuracy: (0.0, 77.14285714285714)\n","Epochs:10 Validation Loss: 0.8571428571428571 Accuracy: (0.0, 85.71428571428571)\n","Epochs:11 Validation Loss: 0.9428571428571428 Accuracy: (0.0, 94.28571428571429)\n","Epochs:12 Validation Loss: 1.0285714285714285 Accuracy: (0.0, 102.85714285714286)\n","Epochs:13 Validation Loss: 1.1142857142857143 Accuracy: (0.0, 111.42857142857143)\n","Epochs:14 Validation Loss: 1.2 Accuracy: (0.0, 120.0)\n","Epochs:15 Validation Loss: 1.2857142857142858 Accuracy: (0.0, 128.57142857142858)\n","Epochs:16 Validation Loss: 1.3714285714285714 Accuracy: (0.0, 137.14285714285714)\n","Epochs:17 Validation Loss: 1.457142857142857 Accuracy: (0.0, 145.71428571428572)\n","Epochs:18 Validation Loss: 1.542857142857143 Accuracy: (0.0, 154.28571428571428)\n","Epochs:19 Validation Loss: 1.6285714285714286 Accuracy: (0.0, 162.85714285714286)\n","Epochs:20 Validation Loss: 1.7142857142857142 Accuracy: (0.0, 171.42857142857142)\n"]}],"source":["# Step 4: Train the model\n","\n","N_EPOCHS = 20\n","loss_list = []\n","accuracy_list = []\n","correct = 0\n","n_test = len(validation_dataset)\n","\n","# Type your code here\n","    # for x,y in train_loader:\n","    #     optimizer.zero_grad()\n","    #     output = model(x)\n","    #     loss_list.append(loss(output,y))\n","    #     optimizer.step()\n","    #     loss_list = []\n","for epoch in range(N_EPOCHS):\n","       \n","          model.eval()\n","          running_losses=0 \n","          for x,y in validation_loader:\n","             output = model(x)\n","             losses = loss(output,y)\n","             _, predicted = output.max(1)\n","             test_loss= running_losses/n_test\n","             correct += predicted.eq(y).sum().item()\n","             accuracy = 100 * correct / n_test     \n","             loss_list.append(test_loss)    \n","             accuracy_list.append(accuracy)\n","          print(f'Epochs:{epoch+1} Validation Loss: {correct / n_test} Accuracy: {test_loss,accuracy}')\n","\n","     "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Step 5</b>: Plot the training loss for each iteration<br> <b>(Your peer reviewer is going to mark based on what you plot here.)</b> \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 5: Plot the loss for training dataset\n","\n","# Type your code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<b>Step 6</b>: Plot the validation accuracy for each epoch<br> <b>(Your peer reviewer is going to mark based on what you plot here.)</b> \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Step 6: Plot the accuracy for valdiation dataset\n","\n","# Type your code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h3 id=\"q33\">Question 3.3: Plot 5 Random Images with their predictions</h3><b>5 points</b>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Create a test dataset using validation data. And, create your own <code>plot_random_image()</code> function to plot 5 random images which index is in the <code>numbers</code> list. Run the function to plot image, print the predicted label and print a string indicate whether it has been correctly classified or mis-classified.<br> <b>(Your peer reviewer is going to mark based on what you plot here.)</b>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot the images with labels\n","\n","look_up = {0: 'predicted: $5'\n","           , 1: 'predicted: $10'\n","           , 2: 'predicted: $20'\n","           , 3: 'predicted: $50'\n","           , 4: 'predicted: $100'\n","           , 5: 'predicted $200'\n","           , 6: 'predicted $500'}\n","random.seed(0)\n","numbers = random.sample(range(70), 5)\n","\n","# Type your code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h3>Question 3.4: Use the second model <code>Densenet121</code> to do the prediction</h3><b>3 points</b>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Repeat the steps in Question 3.1, 3.2 to predict the result using <code>models.densenet121</code> model. Then, print out the last validation accuracy.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p>Steps:</p>\n","<ol>\n","    <li>Load the pre-trained model Densenet</li>\n","    <li>Replace the last classification layer with only 7 classes</li>\n","    <li>Set the configuration (parameters)</li>\n","    <li>Train the model</li>\n","    <li>Print the last validation accuracy</li>\n","</ol>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Hint:\n","<ul>\n","    <li>The second last layer for this model has 1024 outputs.</li>\n","    <li>The last layer for <code>Densenet121</code> can be accessed by <code>model.classifier</code></li>\n","    <li>Use the criterion function <code>nn.CrossEntropyLoss()</code></li>\n","    <li>Train Batch Size: 15; Validation Batch Size: 10</li>\n","    <li>Optimizer: Adam with learning rate 0.003</li>\n","    <li>10 Epoches. Otherwise, it will take too long.</li>\n","</ul>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["You are welcome to try any pattern of setting and find out the best result. Please name the model variable as <code>model_des</code>.<br> <b>(Your peer reviewer is going to mark based on what you print here.)</b>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Use densenet121 to train the model and print out the last validation accuracy.\n","\n","# Type your code here"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h2 id=\"#save\">Save the trained model</h2>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Save the trained model for the following chapters\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'model_des' is not defined","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[46], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Save the model\u001b[39;00m\n\u001b[0;32m      3\u001b[0m torch\u001b[39m.\u001b[39msave(model, \u001b[39m\"\u001b[39m\u001b[39mresnet18_pytorch.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m torch\u001b[39m.\u001b[39msave(model_des, \u001b[39m\"\u001b[39m\u001b[39mdensenet121_pytorch.pt\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[1;31mNameError\u001b[0m: name 'model_des' is not defined"]}],"source":["# Save the model\n","\n","torch.save(model, \"resnet18_pytorch.pt\")\n","torch.save(model_des, \"densenet121_pytorch.pt\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a href=\"https://cocl.us/DLO0320EN_notebook_bott\">\n","    <img src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Assets/Images/Bottom.png\" width=\"750\" alt=\"cognitive class\">\n","</a>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<h2>About the Authors:</h2> \n","\n","<a href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0320ENSkillsNetwork929-2023-01-01\">Joseph Santarcangelo</a> has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Other contributors: <a href=\"https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0320ENSkillsNetwork929-2023-01-01\">Michelle Carey</a>, <a href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\">Mavis Zhou</a>, <a href=\"https://www.linkedin.com/in/yi-leng-yao-84451275/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0320ENSkillsNetwork929-2023-01-01\">Yi Leng Yao</a>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<hr>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Copyright &copy; 2018 <a href=\"cognitiveclass.ai?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu\">cognitiveclass.ai</a>. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0320ENSkillsNetwork929-2023-01-01\">MIT License</a>.\n"]}],"metadata":{"kernelspec":{"display_name":"myenv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":2}
