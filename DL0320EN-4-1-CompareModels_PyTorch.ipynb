{"cells":[{"cell_type":"markdown","metadata":{},"source":["\u003ca href=\"https://cocl.us/DL0320EN_TOP_IMAGE\"\u003e\n","    \u003cimg src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Assets/Images/Top.png\" width=\"750\" alt=\"IBM 10TB Storage\"\u003e\n","\u003c/a\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch1\u003eClassifying European Money Denominations: Comparing Two Models\u003c/h1\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2\u003eTable of Contents\u003c/h2\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003cp\u003eIn this lab you will compare the \u003ccode\u003eResNet18\u003c/code\u003e and \u003ccode\u003eDensenet121\u003c/code\u003e\u003c/p\u003e\n","\u003cul\u003e\n","    \u003cli\u003e\u003ca href=\"#dataset\"\u003eCreate Dataset Class and Object\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#pre\"\u003eLoad Pre-trained Model\u003c/a\u003e\u003c/li\u003e\n","    \u003cli\u003e\u003ca href=\"#analyze\"\u003eAnalyze Models\u003c/a\u003e\u003c/li\u003e\n","\u003c/ul\u003e\n","\n","\u003cp\u003eEstimated Time Needed: \u003cb\u003e25 min\u003c/b\u003e\u003c/p\u003e\n","\u003chr\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2\u003ePreparation\u003c/h2\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ca href=\"https://cocl.us/DL0320EN_storage\"\u003e\n","    \u003cimg src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Assets/Images/ObjectStorage.png\" width=\"750\" alt=\"cognitive class\"\u003e\n","\u003c/a\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["Download the datasets you needed for this lab.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# You can comment out this box when you already have the dataset\n","# Step 1: Ctrl + A : Select all\n","# Step 2: Ctrl + / : Comment out all; if everything selected has been comment out alreaday, then uncomment all\n","\n","# Download test dataset\n","!wget --quiet -O /resources/data/test_data_pytorch.tar.gz https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Datasets/PyTorch/test_data_pytorch.tar.gz\n","!tar -xzf /resources/data/test_data_pytorch.tar.gz -C /resources/data --exclude '.*'"]},{"cell_type":"markdown","metadata":{},"source":["Import the PyTorch Modules needed in the lab.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import PyTorch Modules that will be used in the lab\n","\n","import torch \n","import torchvision.models as models\n","from torch.utils.data import Dataset, DataLoader\n","import pandas\n","from torchvision import transforms\n","import torch.nn as nn\n","torch.manual_seed(0)"]},{"cell_type":"markdown","metadata":{},"source":["Import Non-PyTorch Modules \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import Non-PyTorch Modules that will be used in the lab\n","\n","import time\n","from imageio import imread\n","from matplotlib.pyplot import imshow\n","import matplotlib.pylab as plt\n","import pandas as pd\n","from PIL import Image, ImageDraw, ImageFont\n","import random\n","import numpy as np"]},{"cell_type":"markdown","metadata":{},"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2 id=\"dataset\"\u003eCreate Dataset Class and Object\u003c/h2\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["In this section, you use the dataset class from the last section.\n"]},{"cell_type":"markdown","metadata":{},"source":["The denomination, file name and the class variable for the testing data are stored in the following CSV file.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Url that contains CSV files and image dataset folder\n","\n","test_csv_file = 'https://cocl.us/DL0320EN_TEST_CSV'\n","test_data_dir = '/resources/data/test_data_pytorch/'"]},{"cell_type":"markdown","metadata":{},"source":["Use the dataset class you created in the last lab.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create Dataset Class\n","\n","class Dataset(Dataset):\n","    \n","    # Constructor\n","    def __init__(self, csv_file, data_dir, transform=None):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","        self.data_name = pd.read_csv(csv_file)\n","        self.len = self.data_name.shape[0] \n","    \n","    # Get Length\n","    def __len__(self):\n","        return self.len\n","    \n","    # Getter\n","    def __getitem__(self, idx):\n","        img_name = self.data_dir + self.data_name.iloc[idx, 2]\n","        image = Image.open(img_name)\n","        y = self.data_name.iloc[idx, 3]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, y"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch3\u003eTry\u003c/h3\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["Use the constructor \u003ccode\u003ecompose\u003c/code\u003e to perform the following sequence of transformations in the order they are given, call the object \u003ccode\u003ecomposed\u003c/code\u003e\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Construct the composed object for transforming the image \n","mean = [0.485, 0.456, 0.406]\n","std = [0.229, 0.224, 0.225]\n","trans_step = [transforms.Resize((224, 224)), transforms.ToTensor(), transforms.Normalize(mean, std)]\n","\n","\n","# Type your code here"]},{"cell_type":"markdown","metadata":{},"source":["Double-click \u003cb\u003ehere\u003c/b\u003e for the solution.\n","\u003c!--\n","composed = transforms.Compose(trans_step)\n","--\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["Create a test dataset object using the CSV file stored in the variables the \u003ccode\u003etest_csv_file\u003c/code\u003e. The directories are stored in the variable \u003ccode\u003etest_data_dir\u003c/code\u003e. Set the parameter \u003ccode\u003etransform\u003c/code\u003e to the object \u003ccode\u003ecomposed\u003c/code\u003e. \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Create a test_dataset\n","\n","test_dataset = Dataset(transform=composed\n","                       , csv_file=test_csv_file\n","                       , data_dir=test_data_dir)"]},{"cell_type":"markdown","metadata":{},"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2 id=\"pre\"\u003eLoad Pre-trained Model\u003c/h2\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["Load the \u003ccode\u003eResNet18\u003c/code\u003e and \u003ccode\u003eDensenet121\u003c/code\u003e model you created from the last section \n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load pre-trained model\n","model = torch.load(\"resnet18_pytorch.pt\")\n","model_des = torch.load(\"densenet121_pytorch.pt\")"]},{"cell_type":"markdown","metadata":{},"source":["Print the structures of two models. You need to answer the questions in quiz based on the output here.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Print model structure\n","\n","print(\"ResNet18:\\n\", model)\n","print(\"Densenet121:\\n\", model_des)"]},{"cell_type":"markdown","metadata":{},"source":["Create a data loader object for the test data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Set Data Loader object\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=10)"]},{"cell_type":"markdown","metadata":{},"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2 id=\"analyze\"\u003eAnalyze Models\u003c/h2\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch3\u003eTry\u003c/h3\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["Find the error on the test data using the \u003ccode\u003eResNet18\u003c/code\u003e model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict the data using ResNet18 model and print out accuracy\n","\n","# Type your code here"]},{"cell_type":"markdown","metadata":{},"source":["Double-click \u003cb\u003ehere\u003c/b\u003e for the solution.\n","\u003c!--\n","correct = 0\n","accuracy = 0\n","N = len(test_dataset)\n","for x_test, y_test in test_loader:\n","    model.eval()\n","    z = model(x_test)\n","    _, yhat = torch.max(z.data, 1)\n","    correct += (yhat == y_test).sum().item()\n","accuracy = correct / N\n","print(\"Accuracy using ResNet18: \", accuracy) \n","--\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch3\u003eTry\u003c/h3\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["Find the error on the test data using the \u003ccode\u003eDensenet121\u003c/code\u003e model\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Predict the data using densene model and print out accuracy\n","\n","# Type your code here"]},{"cell_type":"markdown","metadata":{},"source":["Double-click \u003cb\u003ehere\u003c/b\u003e for the solution.\n","\u003c!--\n","correct = 0\n","accuracy = 0\n","N = len(test_dataset)\n","for x_test, y_test in test_loader:\n","    model_des.eval()\n","    z = model_des(x_test)\n","    _, yhat = torch.max(z.data, 1)\n","    correct += (yhat == y_test).sum().item()\n","accuracy = correct / N\n","print(\"Accuracy using Densenet121: \", accuracy) \n","--\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch3\u003eTry\u003c/h3\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["What model performed better on the test data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your answer here"]},{"cell_type":"markdown","metadata":{},"source":["\u003ca href=\"https://cocl.us/DLO0320EN_notebook_bott\"\u003e\n","    \u003cimg src=\"https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/DL0320EN/Assets/Images/Bottom.png\" width=\"750\" alt=\"cognitive class\"\u003e\n","\u003c/a\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003ch2\u003eAbout the Authors:\u003c/h2\u003e \n","\n","\u003ca href=\"https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0320ENSkillsNetwork929-2023-01-01\"\u003eJoseph Santarcangelo\u003c/a\u003e has a PhD in Electrical Engineering, his research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition. Joseph has been working for IBM since he completed his PhD.\n"]},{"cell_type":"markdown","metadata":{},"source":["Other contributors: \u003ca href=\"https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0320ENSkillsNetwork929-2023-01-01\"\u003eMichelle Carey\u003c/a\u003e, \u003ca href=\"www.linkedin.com/in/jiahui-mavis-zhou-a4537814a\"\u003eMavis Zhou\u003c/a\u003e, \u003ca href=\"https://www.linkedin.com/in/yi-leng-yao-84451275/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0320ENSkillsNetwork929-2023-01-01\"\u003eYi Leng Yao\u003c/a\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["\u003chr\u003e\n"]},{"cell_type":"markdown","metadata":{},"source":["Copyright \u0026copy; 2018 \u003ca href=\"cognitiveclass.ai?utm_source=bducopyrightlink\u0026utm_medium=dswb\u0026utm_campaign=bdu\"\u003ecognitiveclass.ai\u003c/a\u003e. This notebook and its source code are released under the terms of the \u003ca href=\"https://bigdatauniversity.com/mit-license/?utm_medium=Exinfluencer\u0026utm_source=Exinfluencer\u0026utm_content=000026UJ\u0026utm_term=10006555\u0026utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0320ENSkillsNetwork929-2023-01-01\"\u003eMIT License\u003c/a\u003e.\n"]}],"metadata":{"kernelspec":{"display_name":"Python","language":"python","name":"conda-env-python-py"},"language_info":{"name":""}},"nbformat":4,"nbformat_minor":2}